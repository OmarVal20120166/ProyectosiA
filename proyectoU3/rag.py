import os
import time

# --- IMPORTACIONES DEL MODO DIRECTO (PLAN B) ---
# Estas son las que te funcionaron bien
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_chroma import Chroma
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

def main():
    # Limpiar pantalla inicial
    os.system('cls' if os.name == 'nt' else 'clear')
    
    print("==================================================")
    print("   ü§ñ SISTEMA RAG - PROYECTO 3 (GEN Z & FILOSOF√çA)")
    print("==================================================")
    
    # 1. CARGAR DATOS
    print("üìÇ 1. Cargando base de conocimientos...")
    if not os.path.exists('./datos'):
        print("‚ùå ERROR: No existe la carpeta 'datos'. Ejecuta primero 'preparar_datos_csv.py'")
        return

    loader = DirectoryLoader('./datos', glob="*.txt", loader_cls=TextLoader)
    documents = loader.load()
    print(f"   ‚úÖ {len(documents)} documentos cargados.")

    # 2. PREPARAR TEXTO
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(documents)

    # 3. CREAR BASE DE DATOS (EMBEDDINGS)
    print("üß† 2. Inicializando cerebro vectorial (esto es r√°pido si ya descarg√≥)...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = Chroma.from_documents(texts, embeddings)
    retriever = db.as_retriever(search_kwargs={"k": 4}) 

    # 4. CONECTAR LLM
    print("ü¶ô 3. Conectando con Ollama (Llama 3.2)...")
    llm = ChatOllama(model="llama3.2", temperature=0.3, keep_alive="1h")

    # 5. CREAR LA TUBER√çA (CHAIN)
    template = """Eres un experto investigador en sociolog√≠a digital y filosof√≠a contempor√°nea.
    Responde a la pregunta bas√°ndote EXCLUSIVAMENTE en el siguiente contexto extra√≠do de redes sociales.
    
    Si la respuesta no est√° en el contexto, di "No tengo informaci√≥n suficiente en la base de datos".
    Cita ejemplos si el contexto los tiene.
    
    Contexto:
    {context}
    
    Pregunta del usuario: {question}
    
    Respuesta √∫til y fundamentada:"""
    
    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    print("\n‚úÖ ¬°SISTEMA LISTO! Ya puedes chatear.")
    print("   (Escribe 'salir' o 'exit' para terminar el programa)\n")

    # --- BUCLE DE CHAT INFINITO ---
    while True:
        try:
            # Input del usuario
            pregunta = input("\nüë§ T√∫: ")
            
            # Condici√≥n de salida
            if pregunta.lower() in ['salir', 'exit', 'adios', 'bye']:
                print("\nüëã ¬°Hasta luego! Cerrando sistema.")
                break
            
            if not pregunta.strip():
                continue

            print("ü§ñ IA: Pensando...", end="\r") # Efecto visual simple
            
            # Generar respuesta
            inicio = time.time()
            respuesta = rag_chain.invoke(pregunta)
            tiempo = time.time() - inicio
            
            # Borrar el "Pensando..." y mostrar respuesta
            print(f"ü§ñ IA ({tiempo:.1f}s): {respuesta}")
            print("-" * 50)
            
        except KeyboardInterrupt:
            print("\nüëã Salida forzada.")
            break
        except Exception as e:
            print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()